name: Sitemap Cache Warming

# Allow this workflow to push the small state file back to the repo
permissions:
  contents: write

on:
  schedule:
    - cron: '0 */2 * * *'  # Every 2 hours
  workflow_dispatch:  # Manual trigger button

jobs:
  warm-cache:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      CF_ALLOWED_TOKEN: ${{ secrets.CF_ALLOWED_TOKEN }}

    steps:
      - name: Checkout (needed so we can read/write the state file)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Fetch and warm entire sitemap
        run: |
          #!/usr/bin/env bash
          set -euo pipefail

          # Custom UA (do not include "curl" or other blocked tokens)
          CUSTOM_UA="shopsalesja-Cache-Warming-Bot/1.0"

          SITEMAP_URL="https://shopsalesja.com/sitemap.xml"
          MAX_URLS_PER_RUN=5
          FRESHNESS_DAYS=60
          STATE_DIR="warming-state"
          STATE_FILE="$STATE_DIR/warming-state.txt"
          TEMP_SITEMAP="sitemap.tmp.xml"

          mkdir -p "$STATE_DIR"

          # Ensure a placeholder state file exists so later steps / commits have something
          if [ ! -f "$STATE_FILE" ]; then
            echo "0" > "$STATE_FILE"
          fi

          echo "üî• Starting sitemap cache warming..."
          echo ""

          # Download sitemap (include bypass header + custom UA)
          echo "üì° Fetching sitemap from $SITEMAP_URL..."
          curl -sL -A "$CUSTOM_UA" -H "X-Bypass: $CF_ALLOWED_TOKEN" "$SITEMAP_URL" -o "$TEMP_SITEMAP" || {
            echo "‚ùå Failed to download sitemap"
            exit 1
          }

          # Debug: show head of sitemap to help troubleshooting
          echo "---- sitemap.tmp.xml head ----"
          head -n 40 "$TEMP_SITEMAP" | sed 's/^/    /'
          echo "---- end head ----"
          echo ""

          # Detect Cloudflare challenge / block HTML and fail with guidance
          matches=$(grep -niE "Attention Required|Sorry, you have been blocked|cf-wrapper|enable_cookies|cloudflare" "$TEMP_SITEMAP" || true)
          if [ -n "$matches" ]; then
            echo "‚ùå Cloudflare challenge or block detected in downloaded content. The runner is being blocked."
            echo "üëâ Make an Allow rule in Cloudflare for the header X-Bypass (see repo secrets & cloudflare rule instructions)."
            echo ""
            echo "Matching lines:"
            echo "$matches" | sed 's/^/    /'
            echo ""
            echo "---- full head for debugging ----"
            head -n 200 "$TEMP_SITEMAP" | sed 's/^/    /'
            echo "---- end ----"
            exit 1
          fi

          # Aggregate <loc> entries. Handle sitemapindex (child .xml sitemaps) by fetching them.
          LOCS_TOP="$(mktemp)"
          AGG="$(mktemp)"
          # Extract locs from top-level sitemap
          sed -n 's/.*<loc>\(.*\)<\/loc>.*/\1/p' "$TEMP_SITEMAP" > "$LOCS_TOP" || true

          # If there are child sitemap .xml links, fetch them and aggregate their <loc>s
          if grep -qiE '\.xml$' "$LOCS_TOP"; then
            echo "üîÅ Found child sitemap(s); fetching and aggregating..."
            while IFS= read -r sm; do
              # skip empty lines
              [ -z "${sm// }" ] && continue
              if echo "$sm" | grep -qiE '\.xml$'; then
                echo "  ‚Ü≥ fetching $sm"
                child_tmp="$(mktemp)"
                if curl -sL -A "$CUSTOM_UA" -H "X-Bypass: $CF_ALLOWED_TOKEN" "$sm" -o "$child_tmp"; then
                  sed -n 's/.*<loc>\(.*\)<\/loc>.*/\1/p' "$child_tmp" >> "$AGG" || true
                else
                  echo "    ‚ö†Ô∏è  Failed to fetch child sitemap $sm"
                fi
                rm -f "$child_tmp"
              fi
            done < "$LOCS_TOP"
            # Also add any non-xml locs that might be present in the top-level (rare)
            grep -viE '\.xml$' "$LOCS_TOP" >> "$AGG" || true
          else
            # No child sitemaps: use the locs from the one file
            cat "$LOCS_TOP" >> "$AGG" || true
          fi

          # Prepare final URL list: normalize line endings and remove blanks
          URLS=$(cat "$AGG" | tr -d '\r' | sed '/^\s*$/d' || true)

          # Cleanup temp files
          rm -f "$LOCS_TOP" "$AGG" || true

          # Filter out typical asset extensions (images, css, js, pdf) but keep page HTML URLs
          URLS=$(echo "$URLS" | grep -viE '\.(xml|jpg|jpeg|png|gif|webp|css|js|pdf)$' || true)

          # Normalize URLs to lowercase paths (align with Worker canonicalization)
          URLS=$(echo "$URLS" | awk '
          {
            line = $0
            if (index(line, "://") == 0) { print line; next }
            split(line, protoSplit, "://")
            proto = protoSplit[1]
            rest = protoSplit[2]
            host = rest
            pathAndAfter = ""
            slashPos = index(rest, "/")
            if (slashPos > 0) {
              host = substr(rest, 1, slashPos-1)
              pathAndAfter = substr(rest, slashPos)
            }
            if (pathAndAfter == "") {
              print proto "://" host
              next
            }
            qPos = index(pathAndAfter, "?")
            hPos = index(pathAndAfter, "#")
            sepPos = 0
            if (qPos > 0 && hPos > 0) {
              sepPos = (qPos < hPos ? qPos : hPos)
            } else if (qPos > 0) {
              sepPos = qPos
            } else if (hPos > 0) {
              sepPos = hPos
            }
            if (sepPos > 0) {
              pathOnly = substr(pathAndAfter, 1, sepPos-1)
              tail = substr(pathAndAfter, sepPos)
            } else {
              pathOnly = pathAndAfter
              tail = ""
            }
            split(pathOnly, segs, "/")
            newPath = ""
            for (i = 1; i <= length(segs); i++) {
              seg = segs[i]
              if (i == 1) {
                newPath = (seg == "" ? "/" : "/" tolower(seg))
              } else {
                newPath = newPath "/" tolower(seg)
              }
            }
            gsub(/\/+/, "/", newPath)
            print proto "://" host newPath tail
          }' || true)

          if [ -z "$URLS" ]; then
            echo "‚ö†Ô∏è  No URLs found in sitemap (or filtered out). Exiting."
            exit 0
          fi

          TOTAL_URLS=$(echo "$URLS" | wc -l | tr -d ' ')
          echo "üìã Found $TOTAL_URLS URLs in sitemap (after filtering)"
          echo ""

          # Load state from previous run (file is in repo after checkout)
          if [ -f "$STATE_FILE" ]; then
            CURRENT_INDEX=$(cat "$STATE_FILE" | tr -d '\r\n' || echo "0")
            if ! [[ "$CURRENT_INDEX" =~ ^[0-9]+$ ]]; then
              echo "‚ö†Ô∏è  Invalid state value in $STATE_FILE; resetting to 0"
              CURRENT_INDEX=0
            else
              echo "üìç Resuming from index $CURRENT_INDEX"
            fi
          else
            CURRENT_INDEX=0
            echo "üÜï Starting new warming cycle"
          fi
          echo ""

          # Guard: if CURRENT_INDEX >= TOTAL_URLS reset to 0
          if [ "$CURRENT_INDEX" -ge "$TOTAL_URLS" ]; then
            echo "üîÑ Saved index beyond total URLs; resetting to 0"
            CURRENT_INDEX=0
          fi

          # Select batch of URLs
          START_LINE=$((CURRENT_INDEX + 1))
          BATCH_URLS=$(echo "$URLS" | tail -n +"$START_LINE" | head -n "$MAX_URLS_PER_RUN" || true)
          BATCH_COUNT=$(echo "$BATCH_URLS" | sed '/^\s*$/d' | wc -l | tr -d ' ')

          if [ -z "$BATCH_URLS" ] || [ "$BATCH_COUNT" -eq 0 ]; then
            echo "üîÑ Reached end of sitemap or no batch available, restarting from beginning"
            CURRENT_INDEX=0
            START_LINE=1
            BATCH_URLS=$(echo "$URLS" | head -n "$MAX_URLS_PER_RUN")
            BATCH_COUNT=$(echo "$BATCH_URLS" | sed '/^\s*$/d' | wc -l | tr -d ' ')
          fi

          echo "üì¶ Processing batch: URLs $START_LINE to $((CURRENT_INDEX + BATCH_COUNT)) of $TOTAL_URLS"
          echo ""

          # Warm loop
          WARMED=0
          SKIPPED=0
          FAILED=0
          INDEX=0

          # Precompute freshness threshold in seconds
          FRESHNESS_THRESHOLD=$((FRESHNESS_DAYS * 86400))

          while IFS= read -r url; do
            url=$(echo "$url" | tr -d '\r')
            INDEX=$((INDEX + 1))
            echo "[$INDEX/$BATCH_COUNT] Checking: $url"

            # HEAD request follow redirects (include UA + bypass header)
            RESPONSE=$(curl -sI -L -A "$CUSTOM_UA" -H "X-Bypass: $CF_ALLOWED_TOKEN" -w "\n%{http_code}" "$url" 2>/dev/null || echo -e "\n000")
            HTTP_CODE=$(echo "$RESPONSE" | tail -n1 | tr -d '\r')
            HEADERS=$(echo "$RESPONSE" | sed '$d' || true)

            if [ "$HTTP_CODE" != "200" ]; then
              echo "  ‚ö†Ô∏è  Got HTTP $HTTP_CODE - skipping"
              FAILED=$((FAILED + 1))
              echo ""
              continue
            fi

            # Extract Age and CF-Cache-Status from headers (case-insensitive)
            AGE=$(echo "$HEADERS" | awk 'BEGIN{IGNORECASE=1} /^age:/{print $2; exit}' | tr -d '\r' || echo "0")
            CACHE_STATUS=$(echo "$HEADERS" | awk 'BEGIN{IGNORECASE=1} /^cf-cache-status:/{print $2; exit}' | tr -d '\r' || echo "MISS")

            if ! [[ "$AGE" =~ ^[0-9]+$ ]]; then AGE=0; fi
            AGE_DAYS=$((AGE / 86400))

            # Skip if fresh
            if [ "${CACHE_STATUS^^}" = "HIT" ] && [ "$AGE" -lt "$FRESHNESS_THRESHOLD" ]; then
              echo "  ‚è≠Ô∏è  Skipping (edge HIT, ${AGE_DAYS}d old)"
              SKIPPED=$((SKIPPED + 1))
              echo ""
              continue
            fi

            # Warm the URL
            echo "  üî• Warming ($CACHE_STATUS, ${AGE_DAYS}d old)..."

            WARM_RESPONSE=$(curl -sL \
              -A "$CUSTOM_UA" \
              -H "X-Bypass: $CF_ALLOWED_TOKEN" \
              -H "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" \
              -H "Accept-Language: en-US,en;q=0.9" \
              -w "\n%{http_code}" \
              "$url" 2>/dev/null || echo -e "\n000")

            WARM_CODE=$(echo "$WARM_RESPONSE" | tail -n1 | tr -d '\r')

            if [ "$WARM_CODE" = "200" ]; then
              echo "  ‚úÖ Warmed successfully"
              WARMED=$((WARMED + 1))
            else
              echo "  ‚ùå Failed (HTTP $WARM_CODE)"
              FAILED=$((FAILED + 1))
            fi

            echo ""
            sleep 2  # Delay between requests
          done <<< "$BATCH_URLS"

          # Update state for next run
          NEXT_INDEX=$((CURRENT_INDEX + BATCH_COUNT))

          if [ "$NEXT_INDEX" -ge "$TOTAL_URLS" ]; then
            echo "üîÑ Completed full cycle, restarting"
            NEXT_INDEX=0
          fi

          echo "$NEXT_INDEX" > "$STATE_FILE"

          echo ""
          echo "üìä Results:"
          echo "   ‚úÖ Warmed: $WARMED"
          echo "   ‚è≠Ô∏è  Skipped: $SKIPPED"
          echo "   ‚ùå Failed: $FAILED"
          echo ""
          echo "üìç Next run will start from index $NEXT_INDEX of $TOTAL_URLS"
          echo ""
          echo "‚úÖ Cache warming complete!"

      - name: Commit and push state file
        if: always()
        run: |
          # Configure git author (Actions bot)
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          # Stage the state file (no-op if missing)
          git add warming-state/warming-state.txt || true

          # Only commit when there are changes
          if git diff --staged --quiet; then
            echo "No state changes to commit"
          else
            git commit -m "Update warming-state: next_index=$(cat warming-state/warming-state.txt)"
            # push back to the same branch the workflow was triggered on
            git push origin "HEAD:${GITHUB_REF#refs/heads/}"
          fi
