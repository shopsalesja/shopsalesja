name: Sitemap Cache Warming

permissions:
  contents: write

on:
  schedule:
    - cron: '0 */2 * * *'  # Every 2 hours
  workflow_dispatch:

jobs:
  warm-cache:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      CF_ALLOWED_TOKEN: ${{ secrets.CF_ALLOWED_TOKEN }}

    steps:
      - name: Checkout (needed so we can read/write the state file)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Fetch and warm entire sitemap
        run: |
          #!/usr/bin/env bash
          set -euo pipefail

          # Configuration
          CUSTOM_UA="shopsalesja-Cache-Warming-Bot/1.0"
          SITEMAP_URL="https://shopsalesja.com/sitemap.xml"
          MAX_URLS_PER_RUN=10     # user requested
          FRESHNESS_DAYS=60       # keep freshness at 60
          STATE_DIR="warming-state"
          STATE_FILE="$STATE_DIR/warming-state.txt"
          TEMP_SITEMAP="sitemap.tmp.xml"

          mkdir -p "$STATE_DIR"
          if [ ! -f "$STATE_FILE" ]; then echo "0" > "$STATE_FILE"; fi

          echo "üî• Starting sitemap cache warming..."
          echo ""

          # curl with retries + exponential backoff + small jitter for robustness
          curl_with_retry() {
            local url="$1" outfile="$2"
            local max_attempts=3
            local attempt=1
            while [ $attempt -le $max_attempts ]; do
              if curl -sL -A "$CUSTOM_UA" -H "X-Bypass: $CF_ALLOWED_TOKEN" "$url" -o "$outfile"; then
                return 0
              fi
              # backoff + jitter
              sleep $(( (2 ** attempt) + (RANDOM % 3) ))
              attempt=$((attempt + 1))
            done
            return 1
          }

          echo "üì° Fetching sitemap from $SITEMAP_URL..."
          if ! curl_with_retry "$SITEMAP_URL" "$TEMP_SITEMAP"; then
            echo "‚ùå Failed to download sitemap"
            exit 1
          fi

          echo "---- sitemap.tmp.xml head ----"
          head -n 40 "$TEMP_SITEMAP" | sed 's/^/    /'
          echo "---- end head ----"
          echo ""

          # Only treat as Cloudflare challenge if NOT XML and challenge markers present
          is_xml=0
          if head -n 6 "$TEMP_SITEMAP" | grep -qi '<?xml'; then is_xml=1; fi
          if head -n 12 "$TEMP_SITEMAP" | grep -qi '<urlset'; then is_xml=1; fi

          if [ "$is_xml" -eq 0 ]; then
            matches=$(grep -niE "Attention Required|Sorry, you have been blocked|cf-wrapper|enable_cookies|Checking your browser|cf-browser-verification|id=\"challenge-form\"|__cf_chl_jschl_tk__|__cf_chl_" "$TEMP_SITEMAP" || true)
            if [ -n "$matches" ]; then
              echo "‚ùå Cloudflare challenge detected ‚Äî runner blocked"
              echo "Matching lines:"
              echo "$matches" | sed 's/^/    /'
              echo ""
              head -n 200 "$TEMP_SITEMAP" | sed 's/^/    /'
              exit 1
            fi
          fi

          # Extract <loc> values and follow child sitemaps when present
          LOCS_TOP="$(mktemp)"
          AGG="$(mktemp)"
          sed -n 's/.*<loc>\(.*\)<\/loc>.*/\1/p' "$TEMP_SITEMAP" > "$LOCS_TOP" || true

          if grep -qiE '\.xml$' "$LOCS_TOP"; then
            echo "üîÅ Found child sitemap(s); fetching child sitemaps..."
            while IFS= read -r sm; do
              [ -z "${sm// }" ] && continue
              if echo "$sm" | grep -qiE '\.xml$'; then
                echo "  ‚Ü≥ fetching $sm"
                child_tmp="$(mktemp)"
                if curl_with_retry "$sm" "$child_tmp"; then
                  sed -n 's/.*<loc>\(.*\)<\/loc>.*/\1/p' "$child_tmp" >> "$AGG" || true
                else
                  echo "    ‚ö†Ô∏è  Failed to fetch child sitemap $sm"
                fi
                rm -f "$child_tmp"
              fi
            done < "$LOCS_TOP"
            # also include any non-xml locs from top-level (rare)
            grep -viE '\.xml$' "$LOCS_TOP" >> "$AGG" || true
          else
            cat "$LOCS_TOP" >> "$AGG" || true
          fi

          URLS=$(cat "$AGG" | tr -d '\r' | sed '/^\s*$/d' || true)
          rm -f "$LOCS_TOP" "$AGG" || true

          # Filter out common asset extensions
          URLS=$(echo "$URLS" | grep -viE '\.(xml|jpg|jpeg|png|gif|webp|css|js|pdf)$' || true)

          # Normalize URL path segments to lowercase (keeps query/hash as-is)
          URLS=$(echo "$URLS" | awk '
          {
            line = $0
            if (index(line, "://") == 0) { print line; next }
            split(line, protoSplit, "://")
            proto = protoSplit[1]
            rest = protoSplit[2]
            host = rest
            pathAndAfter = ""
            slashPos = index(rest, "/")
            if (slashPos > 0) {
              host = substr(rest, 1, slashPos-1)
              pathAndAfter = substr(rest, slashPos)
            }
            if (pathAndAfter == "") {
              print proto "://" host
              next
            }
            qPos = index(pathAndAfter, "?")
            hPos = index(pathAndAfter, "#")
            sepPos = 0
            if (qPos > 0 && hPos > 0) { sepPos = (qPos < hPos ? qPos : hPos) }
            else if (qPos > 0) { sepPos = qPos }
            else if (hPos > 0) { sepPos = hPos }
            if (sepPos > 0) {
              pathOnly = substr(pathAndAfter, 1, sepPos-1)
              tail = substr(pathAndAfter, sepPos)
            } else {
              pathOnly = pathAndAfter
              tail = ""
            }
            split(pathOnly, segs, "/")
            newPath = ""
            for (i = 1; i <= length(segs); i++) {
              seg = segs[i]
              if (i == 1) { newPath = (seg == "" ? "/" : "/" tolower(seg)) }
              else { newPath = newPath "/" tolower(seg) }
            }
            gsub(/\/+/, "/", newPath)
            print proto "://" host newPath tail
          }' || true)

          if [ -z "$URLS" ]; then
            echo "‚ö†Ô∏è  No URLs found in sitemap (or filtered out). Exiting."
            exit 0
          fi

          TOTAL_URLS=$(echo "$URLS" | wc -l | tr -d ' ')
          echo "üìã Found $TOTAL_URLS URLs in sitemap (after filtering)"
          echo ""

          # Load / validate state index
          if [ -f "$STATE_FILE" ]; then
            CURRENT_INDEX=$(cat "$STATE_FILE" | tr -d '\r\n' || echo "0")
            if ! [[ "$CURRENT_INDEX" =~ ^[0-9]+$ ]]; then
              echo "‚ö†Ô∏è  Invalid state value in $STATE_FILE; resetting to 0"
              CURRENT_INDEX=0
            fi
          else
            CURRENT_INDEX=0
          fi

          if [ "$CURRENT_INDEX" -ge "$TOTAL_URLS" ]; then
            CURRENT_INDEX=0
          fi

          # Pick batch
          START_LINE=$((CURRENT_INDEX + 1))
          BATCH_URLS=$(echo "$URLS" | tail -n +"$START_LINE" | head -n "$MAX_URLS_PER_RUN" || true)
          BATCH_COUNT=$(echo "$BATCH_URLS" | sed '/^\s*$/d' | wc -l | tr -d ' ')

          if [ -z "$BATCH_URLS" ] || [ "$BATCH_COUNT" -eq 0 ]; then
            echo "üîÑ Reached end or no batch available. Restarting from beginning."
            CURRENT_INDEX=0
            START_LINE=1
            BATCH_URLS=$(echo "$URLS" | head -n "$MAX_URLS_PER_RUN")
            BATCH_COUNT=$(echo "$BATCH_URLS" | sed '/^\s*$/d' | wc -l | tr -d ' ')
          fi

          echo "üì¶ Processing batch: URLs $START_LINE to $((CURRENT_INDEX + BATCH_COUNT)) of $TOTAL_URLS"
          echo ""

          # Counters for reporting
          WARMED=0; SKIPPED=0; FAILED=0; INDEX=0
          FRESHNESS_THRESHOLD=$((FRESHNESS_DAYS * 86400))

          # We'll accumulate simple diagnostics to show hits vs misses
          EDGES_HIT=0; EDGES_MISS=0; TIERED_HIT=0; TIERED_MISS=0

          while IFS= read -r url; do
            url=$(echo "$url" | tr -d '\r')
            INDEX=$((INDEX + 1))
            echo "[$INDEX/$BATCH_COUNT] Checking: $url"

            # HEAD: follow redirects, include UA + bypass header
            RESPONSE=$(curl -sI -L -A "$CUSTOM_UA" -H "X-Bypass: $CF_ALLOWED_TOKEN" -w "\n%{http_code}" "$url" 2>/dev/null || echo -e "\n000")
            HTTP_CODE=$(echo "$RESPONSE" | tail -n1 | tr -d '\r')
            HEADERS=$(echo "$RESPONSE" | sed '$d' || true)

            if [ "$HTTP_CODE" != "200" ]; then
              echo "  ‚ö†Ô∏è  Got HTTP $HTTP_CODE - skipping"
              FAILED=$((FAILED + 1))
              echo ""
              sleep $((2 + RANDOM % 4))
              continue
            fi

            # Extract Age, cf-cache-status and tiered status
            AGE=$(echo "$HEADERS" | awk 'BEGIN{IGNORECASE=1} /^age:/{print $2; exit}' | tr -d '\r' || echo "0")
            CACHE_STATUS=$(echo "$HEADERS" | awk 'BEGIN{IGNORECASE=1} /^cf-cache-status:/{print $2; exit}' | tr -d '\r' || echo "MISS")
            TIERED_STATUS=$(echo "$HEADERS" | awk 'BEGIN{IGNORECASE=1} /^cf-cache-tiered-status:/{print $2; exit}' | tr -d '\r' || echo "MISS")

            # Track diagnostics
            if [ "${CACHE_STATUS^^}" = "HIT" ]; then EDGES_HIT=$((EDGES_HIT + 1)); else EDGES_MISS=$((EDGES_MISS + 1)); fi
            if [ "${TIERED_STATUS^^}" = "HIT" ] || [ "${TIERED_STATUS^^}" = "REVALIDATED" ]; then TIERED_HIT=$((TIERED_HIT + 1)); else TIERED_MISS=$((TIERED_MISS + 1)); fi

            if ! [[ "$AGE" =~ ^[0-9]+$ ]]; then AGE=0; fi
            AGE_DAYS=$((AGE / 86400))

            # Skip when Edge is HIT, Age < threshold AND tiered status is not MISS (i.e. tiered has it)
            if [ "${CACHE_STATUS^^}" = "HIT" ] && [ "$AGE" -lt "$FRESHNESS_THRESHOLD" ] && [ "${TIERED_STATUS^^}" != "MISS" ]; then
              echo "  ‚è≠Ô∏è  Skipping (Edge+Tiered HIT, ${AGE_DAYS}d old)"
              SKIPPED=$((SKIPPED + 1))
              echo ""
              sleep $((2 + RANDOM % 4))
              continue
            fi

            # Warm the URL (GET) to populate cache layers
            echo "  üî• Warming (was ${CACHE_STATUS}, ${AGE_DAYS}d old)..."
            WARM_RESPONSE=$(curl -sL -A "$CUSTOM_UA" -H "X-Bypass: $CF_ALLOWED_TOKEN" \
              -H "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" \
              -H "Accept-Language: en-US,en;q=0.9" -w "\n%{http_code}" "$url" 2>/dev/null || echo -e "\n000")
            WARM_CODE=$(echo "$WARM_RESPONSE" | tail -n1 | tr -d '\r')

            if [ "$WARM_CODE" = "200" ]; then
              echo "  ‚úÖ Warmed successfully"
              WARMED=$((WARMED + 1))
            else
              echo "  ‚ùå Failed (HTTP $WARM_CODE)"
              FAILED=$((FAILED + 1))
            fi

            echo ""
            # jittered sleep between requests to avoid burstiness
            sleep $((2 + RANDOM % 4))
          done <<< "$BATCH_URLS"

          # update state file
          NEXT_INDEX=$((CURRENT_INDEX + BATCH_COUNT))
          if [ "$NEXT_INDEX" -ge "$TOTAL_URLS" ]; then NEXT_INDEX=0; fi
          echo "$NEXT_INDEX" > "$STATE_FILE"

          # Final summary and diagnostics
          echo ""
          echo "üìä Results:"
          echo "   ‚úÖ Warmed: $WARMED"
          echo "   ‚è≠Ô∏è  Skipped: $SKIPPED"
          echo "   ‚ùå Failed: $FAILED"
          echo ""
          echo "üìç Next run will start from index $NEXT_INDEX of $TOTAL_URLS"
          echo ""
          echo "üßæ Cache diagnostics (this run batch only):"
          echo "   Edge HITs: $EDGES_HIT"
          echo "   Edge MISSes: $EDGES_MISS"
          echo "   Tiered HITs: $TIERED_HIT"
          echo "   Tiered MISSes: $TIERED_MISS"
          echo ""
          echo "‚úÖ Cache warming complete!"
