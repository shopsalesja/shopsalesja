name: Blog Master Sitemap Cache Warming

permissions:
  contents: write

on:
  schedule:
    - cron: '0 */2 * * *'  # Every 2 hours
  workflow_dispatch:

jobs:
  warm-cache:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    env:
      CF_ALLOWED_TOKEN: ${{ secrets.CF_ALLOWED_TOKEN }}
      GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

    steps:
      - name: Checkout (needed so we can read/write the state file)
        uses: actions/checkout@v4
        with:
          fetch-depth: 0
          persist-credentials: true

      - name: Fetch and warm entire sitemap
        run: |
          #!/usr/bin/env bash
          set -euo pipefail

          # Configuration
          CUSTOM_UA="shopsalesja-Cache-Warming-Bot/1.0"
          SITEMAP_URL="https://blogmaster.shopsalesja.com/sitemap.xml"
          MAX_URLS_PER_RUN=12
          FRESHNESS_DAYS=60
          STATE_DIR="warming-state"
          STATE_FILE="$STATE_DIR/blogmaster-warming-state.txt"
          TEMP_SITEMAP="sitemap.tmp.xml"

          # Curl timeouts (seconds)
          CONNECT_TIMEOUT=10
          MAX_TIME_HEAD=20
          MAX_TIME_GET=60

          mkdir -p "$STATE_DIR"
          if [ ! -f "$STATE_FILE" ]; then echo "0" > "$STATE_FILE"; fi

          echo "üî• Starting sitemap cache warming..."
          echo ""

          # curl with retries for file downloads (sitemaps)
          curl_with_retry() {
            local url="$1" outfile="$2"
            local max_attempts=3
            local attempt=1
            while [ $attempt -le $max_attempts ]; do
              if curl -sL -A "$CUSTOM_UA" -H "X-Bypass: $CF_ALLOWED_TOKEN" --connect-timeout $CONNECT_TIMEOUT --max-time $MAX_TIME_GET "$url" -o "$outfile"; then
                return 0
              fi
              echo "  ‚ö†Ô∏è  Attempt $attempt failed for $url ‚Äî backing off"
              sleep $(( (2 ** attempt) + (RANDOM % 3) ))
              attempt=$((attempt + 1))
            done
            return 1
          }

          # HEAD request wrapper with retry & timeouts; returns headers+code on stdout
          curl_head() {
            local url="$1"
            local max_attempts=2
            local attempt=1
            local out
            while [ $attempt -le $max_attempts ]; do
              out=$(curl -S -sI -L -A "$CUSTOM_UA" -H "X-Bypass: $CF_ALLOWED_TOKEN" \
                --connect-timeout $CONNECT_TIMEOUT --max-time $MAX_TIME_HEAD -w "\n%{http_code}" "$url" 2>&1) || out="$out"
              code=$(echo "$out" | tail -n1 | tr -d '\r')
              if [[ "$code" =~ ^[0-9]{3}$ ]]; then
                echo "$out"
                return 0
              fi
              echo "  ‚ö†Ô∏è  HEAD attempt $attempt failed for $url (no status code). Retrying..."
              sleep $(( (2 ** attempt) + (RANDOM % 2) ))
              attempt=$((attempt + 1))
            done
            echo "${out:-}"
            return 1
          }

          # GET request wrapper with retries & timeouts; returns output (last line should be status)
          curl_get() {
            local url="$1"
            local max_attempts=2
            local attempt=1
            local out code
            while [ $attempt -le $max_attempts ]; do
              out=$(curl -S -sL -A "$CUSTOM_UA" -H "X-Bypass: $CF_ALLOWED_TOKEN" \
                --connect-timeout $CONNECT_TIMEOUT --max-time $MAX_TIME_GET -w "\n%{http_code}" "$url" 2>&1) || out="$out"
              code=$(echo "$out" | tail -n1 | tr -d '\r')
              if [[ "$code" =~ ^[0-9]{3}$ ]]; then
                echo "$out"
                return 0
              fi
              echo "  ‚ö†Ô∏è  GET attempt $attempt failed for $url (no status). Retrying..."
              sleep $(( (2 ** attempt) + (RANDOM % 3) ))
              attempt=$((attempt + 1))
            done
            echo "${out:-}"
            return 1
          }

          echo "üì° Fetching sitemap from $SITEMAP_URL..."
          if ! curl_with_retry "$SITEMAP_URL" "$TEMP_SITEMAP"; then
            echo "‚ùå Failed to download sitemap"
            exit 1
          fi

          echo "---- sitemap.tmp.xml head ----"
          head -n 40 "$TEMP_SITEMAP" | sed 's/^/    /'
          echo "---- end head ----"
          echo ""

          # Precise detection: only treat as Cloudflare challenge if the file is not XML and contains challenge markers
          is_xml=0
          if head -n 6 "$TEMP_SITEMAP" | grep -qi '<?xml'; then is_xml=1; fi
          if head -n 12 "$TEMP_SITEMAP" | grep -qi '<urlset'; then is_xml=1; fi

          if [ "$is_xml" -eq 0 ]; then
            matches=$(grep -niE "Attention Required|Sorry, you have been blocked|cf-wrapper|enable_cookies|Checking your browser|cf-browser-verification|id=\"challenge-form\"|__cf_chl_jschl_tk__|__cf_chl_" "$TEMP_SITEMAP" || true)
            if [ -n "$matches" ]; then
              echo "‚ùå Cloudflare challenge detected ‚Äî runner blocked"
              echo "Matching lines:"
              echo "$matches" | sed 's/^/    /'
              echo ""
              head -n 200 "$TEMP_SITEMAP" | sed 's/^/    /'
              exit 1
            fi
          fi

          # Extract <loc> values and handle child sitemaps
          LOCS_TOP="$(mktemp)"
          AGG="$(mktemp)"
          sed -n 's/.*<loc>\(.*\)<\/loc>.*/\1/p' "$TEMP_SITEMAP" > "$LOCS_TOP" || true

          if grep -qiE '\.xml$' "$LOCS_TOP"; then
            echo "üîÅ Found child sitemap(s); fetching child sitemaps..."
            while IFS= read -r sm; do
              [ -z "${sm// }" ] && continue
              if echo "$sm" | grep -qiE '\.xml$'; then
                echo "  ‚Ü≥ fetching $sm"
                child_tmp="$(mktemp)"
                if curl_with_retry "$sm" "$child_tmp"; then
                  sed -n 's/.*<loc>\(.*\)<\/loc>.*/\1/p' "$child_tmp" >> "$AGG" || true
                else
                  echo "    ‚ö†Ô∏è  Failed to fetch child sitemap $sm (continuing)"
                fi
                rm -f "$child_tmp"
              fi
            done < "$LOCS_TOP"
            grep -viE '\.xml$' "$LOCS_TOP" >> "$AGG" || true
          else
            cat "$LOCS_TOP" >> "$AGG" || true
          fi

          URLS=$(cat "$AGG" | tr -d '\r' | sed '/^\s*$/d' || true)
          rm -f "$LOCS_TOP" "$AGG" || true

          # Filter out common asset extensions
          URLS=$(echo "$URLS" | grep -viE '\.(xml|jpg|jpeg|png|gif|webp|css|js|pdf)$' || true)

          # Normalize path segments to lowercase
          URLS=$(echo "$URLS" | awk '
          {
            line = $0
            if (index(line, "://") == 0) { print line; next }
            split(line, protoSplit, "://")
            proto = protoSplit[1]
            rest = protoSplit[2]
            host = rest
            pathAndAfter = ""
            slashPos = index(rest, "/")
            if (slashPos > 0) {
              host = substr(rest, 1, slashPos-1)
              pathAndAfter = substr(rest, slashPos)
            }
            if (pathAndAfter == "") {
              print proto "://" host
              next
            }
            qPos = index(pathAndAfter, "?")
            hPos = index(pathAndAfter, "#")
            sepPos = 0
            if (qPos > 0 && hPos > 0) { sepPos = (qPos < hPos ? qPos : hPos) }
            else if (qPos > 0) { sepPos = qPos }
            else if (hPos > 0) { sepPos = hPos }
            if (sepPos > 0) {
              pathOnly = substr(pathAndAfter, 1, sepPos-1)
              tail = substr(pathAndAfter, sepPos)
            } else {
              pathOnly = pathAndAfter
              tail = ""
            }
            split(pathOnly, segs, "/")
            newPath = ""
            for (i = 1; i <= length(segs); i++) {
              seg = segs[i]
              if (i == 1) { newPath = (seg == "" ? "/" : "/" tolower(seg)) }
              else { newPath = newPath "/" tolower(seg) }
            }
            gsub(/\/+/, "/", newPath)
            print proto "://" host newPath tail
          }' || true)

          if [ -z "$URLS" ]; then
            echo "‚ö†Ô∏è  No URLs found in sitemap (or filtered out). Exiting."
            exit 0
          fi

          TOTAL_URLS=$(echo "$URLS" | wc -l | tr -d ' ')
          echo "üìã Found $TOTAL_URLS URLs in sitemap (after filtering)"
          echo ""

          # Load / validate state index
          if [ -f "$STATE_FILE" ]; then
            CURRENT_INDEX=$(cat "$STATE_FILE" | tr -d '\r\n' || echo "0")
            if ! [[ "$CURRENT_INDEX" =~ ^[0-9]+$ ]]; then
              echo "‚ö†Ô∏è  Invalid state value in $STATE_FILE; resetting to 0"
              CURRENT_INDEX=0
            fi
          else
            CURRENT_INDEX=0
          fi

          if [ "$CURRENT_INDEX" -ge "$TOTAL_URLS" ]; then
            CURRENT_INDEX=0
          fi

          # Pick batch
          START_LINE=$((CURRENT_INDEX + 1))
          BATCH_URLS=$(echo "$URLS" | tail -n +"$START_LINE" | head -n "$MAX_URLS_PER_RUN" || true)
          BATCH_COUNT=$(echo "$BATCH_URLS" | sed '/^\s*$/d' | wc -l | tr -d ' ')

          if [ -z "$BATCH_URLS" ] || [ "$BATCH_COUNT" -eq 0 ]; then
            echo "üîÑ Reached end or no batch available. Restarting from beginning."
            CURRENT_INDEX=0
            START_LINE=1
            BATCH_URLS=$(echo "$URLS" | head -n "$MAX_URLS_PER_RUN")
            BATCH_COUNT=$(echo "$BATCH_URLS" | sed '/^\s*$/d' | wc -l | tr -d ' ')
          fi

          echo "üì¶ Processing batch: URLs $START_LINE to $((CURRENT_INDEX + BATCH_COUNT)) of $TOTAL_URLS"
          echo ""

          # Counters for reporting
          WARMED=0; SKIPPED=0; FAILED=0; INDEX=0
          FRESHNESS_THRESHOLD=$((FRESHNESS_DAYS * 86400))

          # Diagnostics
          EDGES_HIT=0; EDGES_MISS=0; TIERED_HIT=0; TIERED_MISS=0

          while IFS= read -r url; do
            url=$(echo "$url" | tr -d '\r')
            INDEX=$((INDEX + 1))
            echo "[$INDEX/$BATCH_COUNT] Checking: $url"

            # Use HEAD wrapper with timeouts & retry
            HEAD_OUT=$(curl_head "$url") || {
              echo "  ‚ö†Ô∏è  HEAD request failed for $url (timed out or network issue). Marking as failed and continuing."
              FAILED=$((FAILED + 1))
              sleep $((2 + RANDOM % 4))
              continue
            }

            HTTP_CODE=$(echo "$HEAD_OUT" | tail -n1 | tr -d '\r')
            HEADERS=$(echo "$HEAD_OUT" | sed '$d' || true)

            if [ "$HTTP_CODE" != "200" ]; then
              echo "  ‚ö†Ô∏è  Got HTTP $HTTP_CODE - skipping"
              FAILED=$((FAILED + 1))
              echo ""
              sleep $((2 + RANDOM % 4))
              continue
            fi

            # Extract Age, cf-cache-status and tiered status
            AGE=$(echo "$HEADERS" | awk 'BEGIN{IGNORECASE=1} /^age:/{print $2; exit}' | tr -d '\r' || echo "0")
            CACHE_STATUS=$(echo "$HEADERS" | awk 'BEGIN{IGNORECASE=1} /^cf-cache-status:/{print $2; exit}' | tr -d '\r' || echo "MISS")
            TIERED_STATUS=$(echo "$HEADERS" | awk 'BEGIN{IGNORECASE=1} /^cf-cache-tiered-status:/{print $2; exit}' | tr -d '\r' || echo "MISS")

            # Track diagnostics
            if [ "${CACHE_STATUS^^}" = "HIT" ]; then EDGES_HIT=$((EDGES_HIT + 1)); else EDGES_MISS=$((EDGES_MISS + 1)); fi
            if [ "${TIERED_STATUS^^}" = "HIT" ] || [ "${TIERED_STATUS^^}" = "REVALIDATED" ]; then TIERED_HIT=$((TIERED_HIT + 1)); else TIERED_MISS=$((TIERED_MISS + 1)); fi

            if ! [[ "$AGE" =~ ^[0-9]+$ ]]; then AGE=0; fi
            AGE_DAYS=$((AGE / 86400))

            # Skip when Edge is HIT, Age < threshold AND tiered status is not MISS
            if [ "${CACHE_STATUS^^}" = "HIT" ] && [ "$AGE" -lt "$FRESHNESS_THRESHOLD" ] && [ "${TIERED_STATUS^^}" != "MISS" ]; then
              echo "  ‚è≠Ô∏è  Skipping (Edge+Tiered HIT, ${AGE_DAYS}d old)"
              SKIPPED=$((SKIPPED + 1))
              echo ""
              sleep $((2 + RANDOM % 4))
              continue
            fi

            # Warm the URL (GET) to populate cache layers using curl_get wrapper
            echo "  üî• Warming (was ${CACHE_STATUS}, ${AGE_DAYS}d old)..."
            GET_OUT=$(curl_get "$url") || {
              echo "  ‚ùå GET failed or timed out for $url"
              FAILED=$((FAILED + 1))
              sleep $((2 + RANDOM % 4))
              continue
            }

            WARM_CODE=$(echo "$GET_OUT" | tail -n1 | tr -d '\r')
            if [ "$WARM_CODE" = "200" ]; then
              echo "  ‚úÖ Warmed successfully"
              WARMED=$((WARMED + 1))
            else
              echo "  ‚ùå Failed (HTTP $WARM_CODE)"
              FAILED=$((FAILED + 1))
            fi

            echo ""
            # jittered sleep
            sleep $((2 + RANDOM % 4))
          done <<< "$BATCH_URLS"

          # update state file
          NEXT_INDEX=$((CURRENT_INDEX + BATCH_COUNT))
          if [ "$NEXT_INDEX" -ge "$TOTAL_URLS" ]; then NEXT_INDEX=0; fi
          echo "$NEXT_INDEX" > "$STATE_FILE"

          # Final summary and diagnostics
          echo ""
          echo "üìä Results:"
          echo "   ‚úÖ Warmed: $WARMED"
          echo "   ‚è≠Ô∏è  Skipped: $SKIPPED"
          echo "   ‚ùå Failed: $FAILED"
          echo ""
          echo "üìç Next run will start from index $NEXT_INDEX of $TOTAL_URLS"
          echo ""
          echo "üßæ Cache diagnostics (this run batch only):"
          echo "   Edge HITs: $EDGES_HIT"
          echo "   Edge MISSes: $EDGES_MISS"
          echo "   Tiered HITs: $TIERED_HIT"
          echo "   Tiered MISSes: $TIERED_MISS"
          echo ""
          echo "‚úÖ Cache warming complete!"

      - name: Commit and push state file (robust)
        if: always()
        run: |
          set -euo pipefail
          git config user.name "github-actions[bot]"
          git config user.email "41898282+github-actions[bot]@users.noreply.github.com"

          echo "State file (to commit):"
          cat warming-state/warming-state.txt || true

          git add warming-state/warming-state.txt || true

          if git diff --cached --quiet; then
            echo "No state changes to commit"
            exit 0
          fi

          git commit -m "Update warming-state: next_index=$(cat warming-state/warming-state.txt)"

          echo "Attempting git push to ${GITHUB_REF#refs/heads/}..."
          if git push origin "HEAD:${GITHUB_REF#refs/heads/}"; then
            echo "git push succeeded"
            exit 0
          fi

          echo "git push failed ‚Äî falling back to GitHub Contents API using GITHUB_TOKEN"

          owner_repo="$GITHUB_REPOSITORY"
          branch="${GITHUB_REF#refs/heads/}"
          file_path="warming-state/warming-state.txt"

          # Get current file SHA so we can update
          sha=$(curl -s -H "Authorization: token $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$owner_repo/contents/$file_path?ref=$branch" | grep '"sha":' | head -n1 | sed -E 's/.*"sha":\s*"([^"]+)".*/\1/')

          content_b64=$(base64 -w 0 "$file_path")
          msg=$(printf 'Update warming-state: next_index=%s' "$(cat $file_path)")
          payload=$(printf '{"message":"%s","content":"%s","sha":"%s","branch":"%s"}' "$msg" "$content_b64" "$sha" "$branch")

          resp=$(curl -s -X PUT -H "Authorization: token $GITHUB_TOKEN" -H "Accept: application/vnd.github+json" "https://api.github.com/repos/$owner_repo/contents/$file_path" -d "$payload")
          echo "GitHub API response: $resp"

          if echo "$resp" | grep -q '"content":'; then
            echo "State file updated via API successfully"
            exit 0
          fi

          echo "Failed to update state file via API"
          exit 1
